{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANs_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8Y8A//KL3cjxNEZggx3vx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaheepChaudhary/GANs_mnist/blob/main/GANs_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEkdz4JuXzvy"
      },
      "source": [
        "#importing the necessary packages\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as tk\n",
        "from tensorflow.keras.datasets import mnist \n",
        "import imageio\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.layers import Dense,Conv2DTranspose,Conv2D,Dropout,Flatten,MaxPool2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers.Activation import activation\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w82vs-ntOVET"
      },
      "source": [
        "#preprocessing of dataset(normalization and shuffling of images)\n",
        "batch_size = 256\n",
        "\n",
        "dataset = mnist.load_data(path=\"mnist.npz\")\n",
        "x_train,y_train,x_test,y_test = dataset[0][0],dataset[0][1],dataset[1][0],dataset[1][1]\n",
        "x_train = x_train.reshape([x_train.shape[0],28,28,1]).astype('float32')\n",
        "x_train = x_train/255.0\n",
        "#x_train = shuffle(x_train)\n",
        "#x_train = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size)\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aztTGiy9OvDp"
      },
      "source": [
        "#generator model\n",
        "def generator_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(7*7*256))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Reshape((7,7,256)))\n",
        "  model.add(Conv2DTranspose(128,kernel_size=(5,5),strides=(1, 1), padding='same', use_bias=False,activation = 'relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2DTranspose(64,kernel_size=(5,5),strides=(2, 2), padding='same', use_bias=False,activation = 'relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2DTranspose(1,kernel_size=(5,5),strides=(2, 2), padding='same', use_bias=False,activation = 'tanh'))\n",
        "  return model\n",
        "\n",
        "generator = generator_model()  "
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPw9BiyjO4RW"
      },
      "source": [
        "#discriminator model\n",
        "def discriminator_model():\n",
        "  model = Sequential()  \n",
        "  model.add(Conv2D(64,kernel_size = (3,3),activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(MaxPool2D(pool_size=(2,2)))\n",
        "  model.add(Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(MaxPool2D(pool_size=(2,2)))\n",
        "  model.add(Conv2D(256,kernel_size = (3,3),activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(MaxPool2D(pool_size=(2,2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(200,activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(100,activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(50,activation='relu'))\n",
        "  model.add(Dense(1,activation='relu'))\n",
        "  return model\n",
        "\n",
        "discriminator = discriminator_model()   "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJWI2sE1O9nw"
      },
      "source": [
        "#optimizers\n",
        "gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "dis_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPSlOhiLO-zB"
      },
      "source": [
        "#generative_loss\n",
        "def gen_loss(fake):\n",
        "  return tk.losses.binary_crossentropy(tf.zeros_like(fake),fake,from_logits = True)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqPfKuQiPCbC"
      },
      "source": [
        "#discriminator_loss\n",
        "def dis_loss(fake,original):\n",
        "  real_loss = tk.losses.binary_crossentropy(tf.ones_like(original),original,from_logits = True)\n",
        "  fake_loss = tk.losses.binary_crossentropy(tf.zeros_like(fake),fake,from_logits = True)  \n",
        "  total_loss= real_loss + fake_loss\n",
        "  return total_loss  "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpVvDkxKPH5N"
      },
      "source": [
        "#training_Step\n",
        "no_of_examples = 16\n",
        "noise_dim = 100\n",
        "\n",
        "def train_step(images,batch_size):\n",
        "  t = batch_size\n",
        "  noise = tf.random.normal([batch_size,noise_dim])\n",
        "  \n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
        "    gen_images = generator(noise,training = True)\n",
        "    print(np.array(images).shape)\n",
        "    original = discriminator(images,training = True)\n",
        "    print(np.array(gen_images).shape) \n",
        "    fake = discriminator(gen_images,training = True)\n",
        "    generator_loss,discriminator_loss = gen_loss(fake),dis_loss(fake,original)\n",
        "    gen_gradient,dis_gradient = gen_tape.gradient(generator_loss,generator.trainable_variables),dis_tape.gradient(discriminator_loss,discriminator.trainable_variables)\n",
        "\n",
        "  gen_optimizer.apply_gradients(zip(gen_gradient,generator.trainable_variables))\n",
        "  dis_optimizer.apply_gradients(zip(dis_gradient,discriminator.trainable_variables))  "
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65qlLj_JPNf4"
      },
      "source": [
        "#training_function\n",
        "epochs = 50\n",
        "batch_size = 256\n",
        "\n",
        "def train(dataset,epochs,batch_size):\n",
        "  for i in range(epochs):\n",
        "    for batch in dataset:\n",
        "      train_step(batch,batch_size)  \n",
        "\n",
        "    display.clear_output(wait = True)\n",
        "    noise = tf.random.normal([no_of_examples,noise_dim])\n",
        "    generate_and_save_image(generator,epoch+1,noise)\n",
        "\n",
        "  display.clear_output(wait = True)\n",
        "  generate_and_save_image(generator,epochs,noise)\n",
        "  print(\"The number of epoch is \",epoch)\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKWoR_LTPQ2a"
      },
      "source": [
        "#plotting and saving image function\n",
        "def generate_and_save_image(model,epoch,noise):\n",
        "  images = generator(noise,trainable = False)\n",
        "  fig = plt.figure(figsize = (4,4))\n",
        "  for i in range(len(images)):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.imshow(images[i])\n",
        "  plt.show()\n",
        "  plt.savefig('/content/images/epoch'+str(epoch)+'.png') \n",
        "   "
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VszWgn0lSXUS",
        "outputId": "ac9c8ed9-bd3f-4da4-885d-612682123879",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "#training occurs\n",
        "train(dataset,epochs,batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n",
            "(256, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ww8rlKUG0o6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}